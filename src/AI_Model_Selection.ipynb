{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy ver.: 1.25.1\n",
      "pandas ver.: 1.5.3\n",
      "tensorflow ver.: 2.12.0\n",
      "keras ver.: 2.12.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import datetime as dt\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import CSVLogger, EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print('numpy ver.: ' + np.__version__)\n",
    "print('pandas ver.: ' + pd.__version__)\n",
    "print('tensorflow ver.: ' + tf.__version__) \n",
    "print('keras ver.: ' + keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Section: Merging and Preprocessing Data  \n",
    "  \n",
    "This code section performs the following tasks:  \n",
    "  \n",
    "1. Load the `merged_df` DataFrame from a pickle file if it exists. If not, load the `ann_with_apc_pcount_data` and `ANN_data` DataFrames from their respective pickle files.  \n",
    "  \n",
    "2. Preprocess the `ann_with_apc_pcount_data` DataFrame by splitting the 'i' column into three new columns: 'TripId', 'Date', and 'Stop_id'. Then, drop the original 'i' column.  \n",
    "  \n",
    "3. Convert the 'TripId' and 'stopOrder' columns in both `ann_with_apc_pcount_data` and `ANN_data` DataFrames to string data type.  \n",
    "  \n",
    "4. Merge the `ANN_data` and `ann_with_apc_pcount_data` DataFrames on 'TripId' and 'stopOrder' columns, and replace missing values with -1.  \n",
    "  \n",
    "5. Drop unnecessary columns from the merged DataFrame and calculate the average link time for each link.  \n",
    "  \n",
    "6. Create a dictionary to store the link times for each trip and update the link time values based on the average link times.  \n",
    "  \n",
    "7. Iterate through the merged DataFrame and store the link times up to the current stop in a new column named 'links_refs_till_current_stop'.  \n",
    "  \n",
    "8. Save the final merged DataFrame to a pickle file named `merged_df_final_1.pkl` for future use.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('../finalmodel/data/merged_df_final_1.pkl', 'rb') as file:\n",
    "        merged_df = pickle.load(file)\n",
    "except:\n",
    "    with open('../finalmodel/data/aia/Final_Data.pkl', 'rb') as file:\n",
    "        ann_with_apc_pcount_data = pickle.load(file)\n",
    "\n",
    "    # Split the 'i' column into three new columns\n",
    "    ann_with_apc_pcount_data[['TripId', 'Date', 'Stop_id']] = ann_with_apc_pcount_data['i'].str.split('_', expand=True)\n",
    "\n",
    "    # Drop the original 'i' column\n",
    "    ann_with_apc_pcount_data.drop(columns=['i'], inplace=True)\n",
    "\n",
    "    with open('../finalmodel/data/matan/ANN_Complete_data2.pkl', 'rb') as file:\n",
    "        ANN_data = pickle.load(file)\n",
    "\n",
    "    ANN_data['TripId'] = ANN_data['TripId'].astype(str)\n",
    "    ANN_data['stopOrder'] = ANN_data['stopOrder'].astype(str)\n",
    "\n",
    "    ann_with_apc_pcount_data['TripId'] = ann_with_apc_pcount_data['TripId'].astype(str)\n",
    "    ann_with_apc_pcount_data['stopOrder'] = ann_with_apc_pcount_data['stopOrder'].astype(str)\n",
    "\n",
    "    merged_df = pd.merge(ANN_data, ann_with_apc_pcount_data, on=['TripId', 'stopOrder'], how='left')\n",
    "    \n",
    "    merged_df.replace({np.nan: -1, pd.NaT: -1, 'None': -1, 'N/A': -1}, inplace=True)\n",
    "\n",
    "    merged_df = merged_df.drop(columns=['actualArrivalTime_y', 'Linkref_y', 'actualDepartureTime_y'])\n",
    "\n",
    "    avg_times = merged_df.groupby('Linkref_x')['linkTime'].mean().to_dict()\n",
    "    TripIds = merged_df.groupby('TripId')\n",
    "\n",
    "    def drop_negative_values(dictionary):\n",
    "        return {key: value for key, value in dictionary.items() if value >= 0}\n",
    "    avg_times = drop_negative_values(avg_times)\n",
    "\n",
    "    links_time = dict()\n",
    "    for trip_id, group_df in TripIds:\n",
    "        Linkref = list(group_df['Linkref_x'])\n",
    "        linkTime = list(group_df['linkTime'])\n",
    "\n",
    "        for i in range(len(linkTime)):\n",
    "            if linkTime[i] < 0:\n",
    "                if Linkref[i] in avg_times:\n",
    "                    linkTime[i] = avg_times[Linkref[i]]\n",
    "                else:\n",
    "                    linkTime[i] = -1\n",
    "\n",
    "        links_time[trip_id] = linkTime\n",
    "\n",
    "    for index, row in merged_df.iterrows():\n",
    "        trip_id = row['TripId']\n",
    "        stop_order = int(row['stopOrder'])\n",
    "        \n",
    "        if stop_order != 2:\n",
    "            # Get the value from the dictionary up to the 'stopOrder'\n",
    "            value_from_dict = links_time[trip_id][:stop_order-2]\n",
    "                \n",
    "            # Store the value in a new column in the DataFrame\n",
    "            merged_df.at[index, 'links_refs_till_current_stop'] = '_'.join(str(item) for item in value_from_dict)\n",
    "\n",
    "    with open('../finalmodel/data/merged_df_final_1.pkl', 'wb') as file:\n",
    "        pickle.dump(merged_df, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Section: Defining Functions to Extract Relevant Features  \n",
    "  \n",
    "This code section defines three functions to extract relevant features from the input DataFrames:  \n",
    "  \n",
    "1. `get_ann_info(row)`: Extracts features related to the ANN model, including travel time, headway time, link reference ID, arrival and departure times, delay levels, and time periods.  \n",
    "  \n",
    "2. `get_ann_with_apc_pcount_info(row)`: Extracts features related to the ann_with_apc_pcount model, including direction, stop sequence, day of the week, time categories, and previous delay information.  \n",
    "  \n",
    "3. `get_multi_models_info(row)`: Extracts features related to the multi-models approach, including link references till the current stop, ID, and stop order.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ann_info(row):\n",
    "    return row[['K-1_Travel_Time', 'K-2_Travel_Time', 'K-3_Travel_Time', 'Headway_Time',\n",
    "       'K-1_Headway_Time', 'K-2_Headway_Time', 'K-3_Headway_Time', 'LinkrefID',\n",
    "       'ArrivalTimeHour', 'ArrivalTimeMinute', 'ArrivalTimeSecond',\n",
    "       'DepartureTimeHour', 'DepartureTimeMinute', 'DepartureTimeSecond',\n",
    "       'Delay_Level_Level 1', 'Delay_Level_Level 2', 'Delay_Level_Level 3',\n",
    "       'Delay_Level_Level 4', 'Time_Period_Weekday a.m. peak hours',\n",
    "       'Time_Period_Weekday off-peak hours',\n",
    "       'Time_Period_Weekday p.m. peak hours', 'Time_Period_Weekend all-day']]  \n",
    "\n",
    "def get_ann_with_apc_pcount_info(row):\n",
    "    return row[['direction', 'StopSequence', 'DayInWeek_friday',\n",
    "       'DayInWeek_monday', 'DayInWeek_saturday', 'DayInWeek_sunday',\n",
    "       'DayInWeek_thursday', 'DayInWeek_tuesday', 'DayInWeek_wednesday',\n",
    "       'preD1', 'preD2', 'timeCategory_Unknown', 'timeCategory_d1',\n",
    "       'timeCategory_d2', 'timeCategory_d3', 'timeCategory_d4',\n",
    "       'timeCategory_d5', 'timeCategory_d6']] \n",
    "\n",
    "def get_multi_models_info(row):\n",
    "    return row[['links_refs_till_current_stop', 'ID', 'stopOrder']] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code section, we perform preparation of the merged_df to the diff models:\n",
    "\n",
    "1. Import the required libraries, including TensorFlow, Keras, and Scikit-learn.\n",
    "\n",
    "2. Load the ann_with_apc_pcount training data and fit a `StandardScaler` object to it, excluding the 'i' and 'targetTime' columns.\n",
    "\n",
    "3. Load the pre-trained ann_with_apc_pcount and ANN models.\n",
    "\n",
    "4. Define a `DataHolder` class to store information about the lines, routes, and models.\n",
    "\n",
    "5. Define a `build_model` function to create an ANN model with a specified number of input features (number of stops).\n",
    "\n",
    "6. Load line information from a pickle file and create a dictionary of `DataHolder` objects.\n",
    "\n",
    "7. Process a DataFrame called `merged_df`:\n",
    "   - Split the 'LINE_DESC' column by '-' and create a new 'ID' column from the first part of the split.\n",
    "   - Create a new 'DIRECTION' column from the second part of the split.\n",
    "   - Concatenate the 'ID' and 'DIRECTION' columns as the new 'ID' column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "with open('../finalmodel/data/aia/F.Data.pkl', 'rb') as file:\n",
    "        ann_with_apc_pcount_data = pickle.load(file)\n",
    "ann_with_apc_pcount_scaler_normalizer= StandardScaler()\n",
    "ann_with_apc_pcount_scaler_normalizer.fit_transform(ann_with_apc_pcount_data.drop(columns=['i', 'targetTime']))\n",
    "#X_test = ann_with_apc_pcount_scaler_normalizer.transform(X_test_)\n",
    "ann_with_apc_pcount_model = load_model(\"../finalmodel/data/aia/simpleANN_BusTracker_2_64.h5\")\n",
    "ann_model = load_model(\"../finalmodel/data/matan/my_model1.h5\")\n",
    "\n",
    "class DataHolder:\n",
    "    def __init__(self, line_name, links, routs, links_defaults, X, Y, models):\n",
    "        self.line_name = line_name\n",
    "        self.links = links\n",
    "        self.routs = routs\n",
    "        self.links_defaults = links_defaults\n",
    "        self.x_train = X\n",
    "        self.y_train = Y\n",
    "        self.x_test = X\n",
    "        self.y_test = Y\n",
    "        self.models = models\n",
    "\n",
    "def build_model(num_of_stops):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(units=64, input_shape=(num_of_stops,)))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "\n",
    "    model.compile(optimizer=Adam(), loss = \"MAE\", metrics=[])\n",
    "    \n",
    "    return model\n",
    "\n",
    "with open('../finalmodel/data/haim/lines_info.pickle', 'rb') as file:\n",
    "        multi_models = pickle.load(file)\n",
    "multi_models = {data.line_name: data for data in multi_models}\n",
    "\n",
    "# split LINE_DESC column by '-' and take the 0th element to create 'ID' column\n",
    "merged_df['ID'] = merged_df['LINE_DESC'].str.split('-').str[0]\n",
    "# split LINE_DESC column by '-' and take the 1st element to create 'DIRECTION' column\n",
    "merged_df['DIRECTION'] = merged_df['LINE_DESC'].str.split('-').str[1]\n",
    "merged_df['ID'] = merged_df['ID'].astype(str) + '_' + merged_df['DIRECTION'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code section, we perform the following tasks:\n",
    "\n",
    "1. Select a specific row from the `merged_df` DataFrame.\n",
    "\n",
    "2. Extract the ANN, ann_with_apc_pcount, and multi-models information for the selected row using the `get_ann_info`, `get_ann_with_apc_pcount_info`, and `get_multi_models_info` functions.\n",
    "\n",
    "3. Check if the extracted ann_with_apc_pcount row does not contain any -1 or null values:\n",
    "   - Reshape the ann_with_apc_pcount row and scale it using the `ann_with_apc_pcount_scaler_normalizer` object.\n",
    "   - Pass the scaled and reshaped ann_with_apc_pcount row to the pre-trained ann_with_apc_pcount model and print the prediction.\n",
    "\n",
    "4. Check if the extracted ANN row does not contain any -1 or null values:\n",
    "   - Reshape the ANN row and pass it to the pre-trained ANN model.\n",
    "   - Print the prediction from the ANN model.\n",
    "\n",
    "5. Check if the extracted multi-models row does not contain any -1 or null values:\n",
    "   - Split the 'links_refs_till_current_stop' column by '_' and ensure it does not contain any -1 values.\n",
    "   - Check if the multi-models row 'ID' exists in the `multi_models` dictionary.\n",
    "   - Build an ANN model using the `build_model` function with the appropriate input features (number of stops).\n",
    "   - Set the weights of the built model using the pre-trained model from the `multi_models` dictionary.\n",
    "   - Convert the 'links_time' values to float, reshape, and pass them to the built model.\n",
    "   - Print results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\davidfis\\Downloads\\_MBA\\final_project\\Bus-Tracker\\.venv\\lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 297ms/step\n",
      "[[13383.347]]\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "[[23.31433]]\n",
      "1/1 [==============================] - 0s 178ms/step\n",
      "[[0.]]\n"
     ]
    }
   ],
   "source": [
    "# row with all data of all models\n",
    "row = merged_df.iloc[478]\n",
    "\n",
    "ann_row = get_ann_info(row)\n",
    "ann_with_apc_pcount_row = get_ann_with_apc_pcount_info(row)\n",
    "multi_models_row = get_multi_models_info(row)\n",
    "\n",
    "if not (ann_with_apc_pcount_row == -1).any() and not ann_with_apc_pcount_row.isnull().any():\n",
    "    ann_with_apc_pcount_row = np.array(list(ann_with_apc_pcount_row)).reshape(1, -1)\n",
    "    ann_with_apc_pcount_row = ann_with_apc_pcount_scaler_normalizer.transform(ann_with_apc_pcount_row)\n",
    "    print(ann_with_apc_pcount_model.predict(ann_with_apc_pcount_row))\n",
    "    pass\n",
    "\n",
    "if not (ann_row == -1).any() and not ann_row.isnull().any():\n",
    "    print(ann_model.predict(np.array(list(ann_row)).reshape(1, -1)))\n",
    "\n",
    "if not (multi_models_row == -1).any() and not multi_models_row.isnull().any():\n",
    "    links_time = multi_models_row['links_refs_till_current_stop'].split('_')\n",
    "    if -1 not in links_time and multi_models_row['ID'] in multi_models:\n",
    "        model = build_model(int(multi_models_row['stopOrder'])-2)\n",
    "        model.set_weights(multi_models[multi_models_row['ID']].models[int(multi_models_row['stopOrder'])-3])\n",
    "        links_time_float = np.array(links_time).astype(float)\n",
    "        links_time_reshaped = links_time_float.reshape(1, -1)\n",
    "        print(model.predict(links_time_reshaped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "for index, row in merged_df.iterrows():\n",
    "    ann_p = -1\n",
    "    ann_with_apc_pcount_p = -1\n",
    "    multi_p = -1\n",
    "\n",
    "    ann_row = get_ann_info(row)\n",
    "    ann_with_apc_pcount_row = get_ann_with_apc_pcount_info(row)\n",
    "    multi_models_row = get_multi_models_info(row)\n",
    "\n",
    "    if not (ann_with_apc_pcount_row == -1).any() and not ann_with_apc_pcount_row.isnull().any():\n",
    "        ann_with_apc_pcount_row = np.array(list(ann_with_apc_pcount_row)).reshape(1, -1)\n",
    "        ann_with_apc_pcount_row = ann_with_apc_pcount_scaler_normalizer.transform(ann_with_apc_pcount_row)\n",
    "        ann_with_apc_pcount_p = int(ann_with_apc_pcount_model.predict(ann_with_apc_pcount_row)) - row['preD1']\n",
    "\n",
    "    if not (ann_row == -1).any() and not ann_row.isnull().any():\n",
    "        ann_p = ann_model.predict(np.array(list(ann_row)).reshape(1, -1))\n",
    "\n",
    "    if not (multi_models_row == -1).any() and not multi_models_row.isnull().any():\n",
    "        links_time = multi_models_row['links_refs_till_current_stop'].split('_')\n",
    "        if -1 not in links_time and multi_models_row['ID'] in multi_models:\n",
    "            try:\n",
    "                model = build_model(int(multi_models_row['stopOrder'])-2)\n",
    "                model.set_weights(multi_models[multi_models_row['ID']].models[int(multi_models_row['stopOrder'])-3])\n",
    "                links_time_float = np.array(links_time).astype(float)\n",
    "                links_time_reshaped = links_time_float.reshape(1, -1)\n",
    "                multi_p = model.predict(links_time_reshaped)\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Calculate the number of seconds since the start of the day\n",
    "merged_df['targetTime'] = merged_df['OriginAimedDepartureTime'].dt.hour * 3600 + merged_df['OriginAimedDepartureTime'].dt.minute * 60 + merged_df['OriginAimedDepartureTime'].dt.second\n",
    "\n",
    "# Define the function to map targetTime values to categories\n",
    "def categorize_target_time(time_value):\n",
    "    # Define the time intervals for each category in seconds\n",
    "    if 18000 <= time_value < 25200:  # 05:00 AM to 07:59 AM (5 to 7 hours 59 minutes)\n",
    "        return \"d1\"\n",
    "    elif 25200 <= time_value < 32400:  # 08:00 AM to 10:59 AM (8 to 10 hours 59 minutes)\n",
    "        return \"d2\"\n",
    "    elif 32400 <= time_value < 39600:  # 11:00 AM to 01:59 PM (11 to 13 hours 59 minutes)\n",
    "        return \"d3\"\n",
    "    elif 39600 <= time_value < 46800:  # 02:00 PM to 04:59 PM (14 to 16 hours 59 minutes)\n",
    "        return \"d4\"\n",
    "    elif 46800 <= time_value < 54000:  # 05:00 PM to 07:59 PM (17 to 19 hours 59 minutes)\n",
    "        return \"d5\"\n",
    "    elif 54000 <= time_value <= 75599:  # 08:00 PM to 01:59 AM (20 to 23 hours 59 minutes)\n",
    "        return \"d6\"\n",
    "    else:\n",
    "        return \"Unknown\"  # Handle any other cases or outliers\n",
    "\n",
    "# Apply the function to create the new categorical feature 'timeCategory'\n",
    "merged_df['timeCategory'] = merged_df['targetTime'].apply(categorize_target_time)\n",
    "\n",
    "merged_df['timeCategory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = {\n",
    "    'Result 1': [int(r[0]) if int(r[0]) >= 0 else -1 for r in result],\n",
    "    'Result 2': [int(r[1]) if int(r[0]) >= 0 else -1 for r in result],\n",
    "    'Result 3': [int(r[2]) if int(r[0]) >= 0 else -1 for r in result],\n",
    "    'Real Results': list(merged_df['linkTime']),\n",
    "    'time in date': list(merged_df['timeCategory'])\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data1)\n",
    "\n",
    "# Display basic statistics about the data\n",
    "print(df.describe())\n",
    "\n",
    "# Create scatter plots to compare each result against real results\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df, x='Real Results', y='Result 1', label='ANN_VM_And_Pcount (ANN 1)')\n",
    "sns.scatterplot(data=df, x='Real Results', y='Result 2', label='ANN_Road_Approach_Model (ANN 2)')\n",
    "sns.scatterplot(data=df, x='Real Results', y='Result 3', label='Segmented_Task_Model')\n",
    "\n",
    "plt.plot([0, 1000], [0, 1000], color='black', linestyle='dashed', label='Real Results')\n",
    "\n",
    "plt.xlabel('Real Results')\n",
    "plt.ylabel('Results')\n",
    "plt.title('Comparison of Results vs. Real Results')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Create a pairplot to visualize relationships between variables\n",
    "sns.pairplot(df, diag_kind='kde')\n",
    "plt.suptitle('Pairplot of Results and Real Results', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by 'Time Category' and calculate the error\n",
    "grouped_results = df.groupby('time in date')[['Result 1', 'Result 2', 'Result 3']]\n",
    "\n",
    "# Plot the grouped bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "grouped_results.plot(kind='bar')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Time Category')\n",
    "plt.ylabel('Average Result')\n",
    "plt.title('Average Results by Time Category and Result Type')\n",
    "plt.legend(title='Result Type')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
