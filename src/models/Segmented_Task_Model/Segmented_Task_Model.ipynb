{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy ver.: 1.25.1\n",
      "pandas ver.: 1.5.3\n",
      "tensorflow ver.: 2.12.0\n",
      "keras ver.: 2.12.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import datetime as dt\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import CSVLogger, EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "plt.rcParams['axes.facecolor'] = 'w'\n",
    "plt.rcParams['axes.labelcolor'] = 'k'\n",
    "plt.rcParams['axes.edgecolor'] = 'k'\n",
    "plt.rcParams['ytick.color'] = 'k'\n",
    "plt.rcParams['xtick.color'] = 'k'\n",
    "plt.rcParams['grid.color'] = (.7, .7, .7, 0)\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "\n",
    "print('numpy ver.: ' + np.__version__)\n",
    "print('pandas ver.: ' + pd.__version__)\n",
    "print('tensorflow ver.: ' + tf.__version__) \n",
    "print('keras ver.: ' + keras.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/VMs.pickle', 'rb') as file:\n",
    "    VMs_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TripId', 'OPERATOR_ID', 'CLUSTER_ID', 'License_Plate',\n",
       "       'LINE_SHORT_NAME', 'OriginAimedDepartureTime', 'LINE_DESC', 'RouteId',\n",
       "       'Direction', 'Alternative', 'stopOrder', 'actualArrivalTime_x',\n",
       "       'actualDepartureTime_x', 'Linkref_x', 'linkTime', 'time_first_stop(s)',\n",
       "       'Link_travel_time(s)', 'Trip_End', 'timestamp1', 'timestamp2', 'date',\n",
       "       'Arrival_time', 'Departure_time', 'K-1_Travel_Time', 'K-2_Travel_Time',\n",
       "       'K-3_Travel_Time', 'Headway_Time', 'K-1_Headway_Time',\n",
       "       'K-2_Headway_Time', 'K-3_Headway_Time', 'Time_Period',\n",
       "       'average_free_flow_time', 'Average_Bus_Dwelling_Time', 'Bus_Delay',\n",
       "       'Delay_Ratio', 'Delay_Level', 'LinkrefID', 'direction', 'StopSequence',\n",
       "       'DayInWeek_friday', 'DayInWeek_monday', 'DayInWeek_saturday',\n",
       "       'DayInWeek_sunday', 'DayInWeek_thursday', 'DayInWeek_tuesday',\n",
       "       'DayInWeek_wednesday', 'preD1', 'preD2', 'timeCategory_Unknown',\n",
       "       'timeCategory_d1', 'timeCategory_d2', 'timeCategory_d3',\n",
       "       'timeCategory_d4', 'timeCategory_d5', 'timeCategory_d6', 'tripID',\n",
       "       'Linkref_y', 'actualDepartureTime_y', 'actualArrivalTime_y',\n",
       "       'targetTime', 'Date', 'Stop_id', 'Day', 'time'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VMs_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the VMs dataframe by adding 'Day' and 'time' columns, sorting the data, and filtering out rows with TripId equal to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "VMs_df['Day'] = pd.to_datetime(VMs_df['OriginAimedDepartureTime']).dt.date\n",
    "VMs_df['time'] = pd.to_datetime(VMs_df['OriginAimedDepartureTime']).dt.strftime('%H:%M:%S')\n",
    "VMs_df = VMs_df.sort_values(by=['TripId', 'ID', 'Day', 'OriginAimedDepartureTime', 'time', 'stopOrder'])\n",
    "VMs_df = VMs_df[VMs_df['TripId'] != 0]\n",
    "\n",
    "has_nulls = VMs_df.isnull().any().any()\n",
    "has_zeros = (VMs_df['Linkref'] == 0).any()\n",
    "assert has_nulls == False\n",
    "assert has_zeros == False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group VMs dataframe by 'ID' and create DataHolder objects for each line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataHolder:\n",
    "    def __init__(self, line_name, links, routs, links_defaults, X, Y, models):\n",
    "        self.line_name = line_name\n",
    "        self.links = links\n",
    "        self.routs = routs\n",
    "        self.links_defaults = links_defaults\n",
    "        self.x = X\n",
    "        self.y = Y\n",
    "        self.models = models\n",
    "\n",
    "lines_dfs = [group_df for _, group_df in VMs_df.groupby('ID')]\n",
    "lines_info = []\n",
    "\n",
    "for line_df in lines_dfs:\n",
    "    links_unique_values = line_df['Linkref'].unique()\n",
    "    routs_unique_values = line_df['time'].unique()\n",
    "    id_unique_values = line_df['ID'].unique()\n",
    "    assert len(id_unique_values) == 1\n",
    "    lines_info.append(DataHolder(id_unique_values[0], links_unique_values, routs_unique_values, None, None, None, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepper a tensor with default values per line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('../data/default_tensors.pickle', 'rb') as file:\n",
    "        default_tensors = pickle.load(file)\n",
    "except:\n",
    "    VALID_MISS = 5\n",
    "    default_tensors = []\n",
    "    i = 0\n",
    "    for line_df, info in zip(lines_dfs, lines_info):\n",
    "        default_tensor = np.zeros((len(info.links), 1))\n",
    "        for n, (_, day_df) in enumerate(line_df.groupby('Day')):\n",
    "            for (_, group_df) in day_df.groupby('time'):\n",
    "                for (_, row) in group_df.iterrows():\n",
    "                    if row['time'] in info.routs and row['Linkref'] in info.links and len(group_df) > (len(info.links) - VALID_MISS):\n",
    "                        link_index = np.where(info.links == row['Linkref'])[0]\n",
    "\n",
    "                        # FIXME <something> / 2 it a moving average\n",
    "                        if row['linkTime'] > 0:\n",
    "                            default_tensor[(link_index), 0] = (default_tensor[(link_index), 0] + row['linkTime']) / 2\n",
    "                            \n",
    "        default_tensors.append(default_tensor)\n",
    "    \n",
    "    with open('../data/default_tensors.pickle', 'wb') as file:\n",
    "        pickle.dump(default_tensors, file)\n",
    "\n",
    "for i, info in enumerate(lines_info):\n",
    "    info.links_defaults = default_tensors[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save link time info for every line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('../data/link_time_info.pickle', 'rb') as file:\n",
    "        link_time_info = pickle.load(file)\n",
    "except:\n",
    "    link_time_info = []\n",
    "\n",
    "    for line_df, info in zip(lines_dfs, lines_info):\n",
    "        line_link_time = []\n",
    "        for n, (_, day_df) in enumerate(line_df.groupby('Day')):\n",
    "            for (_, group_df) in day_df.groupby('time'):\n",
    "                    all_links_time = ([0] * (len(info.links_defaults)))\n",
    "\n",
    "                    for (_, row) in group_df.iterrows():\n",
    "                        if row['time'] in info.routs and row['Linkref'] in info.links and len(group_df) > (len(info.links) - VALID_MISS):\n",
    "                            link_index = np.where(info.links == row['Linkref'])[0]\n",
    "                            if row['linkTime'] > 0:\n",
    "                                all_links_time[link_index[0]] = row['linkTime']\n",
    "\n",
    "                    for link in range(len(info.links)):\n",
    "                        if all_links_time[link] == 0:\n",
    "                            all_links_time[link] = info.links_defaults[link, 0]\n",
    "\n",
    "                    line_link_time.append(all_links_time)\n",
    "                        \n",
    "        link_time_info.append(line_link_time)\n",
    "    \n",
    "    with open('../data/link_time_info.pickle', 'wb') as file:\n",
    "        pickle.dump(link_time_info, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare input (X) and output (Y) data for each DataHolder object (we one per line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, info in enumerate(lines_info):\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for j in range(len(info.links)):\n",
    "        x = []\n",
    "        y = []\n",
    "        for rout in link_time_info[i]:\n",
    "            if not rout:\n",
    "                continue\n",
    "            if j == 0:\n",
    "                continue\n",
    "            else:\n",
    "                x.append(rout[:j])\n",
    "                y.append(rout[j])\n",
    "        \n",
    "        if not x or not y:\n",
    "            continue\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    \n",
    "    info.x = X\n",
    "    info.y = Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to build a neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_of_stops):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(units=64, input_shape=(num_of_stops,)))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "\n",
    "    model.compile(optimizer=Adam(), loss = \"MAE\", metrics=[])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train neural network models for each DataHolder object and save the models' weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, info in enumerate(lines_info):\n",
    "    tmp_models = []\n",
    "    print(i, info.line_name)\n",
    "    for j in range(len(info.links)):\n",
    "\n",
    "        if j == len(info.links) - 1:\n",
    "            # can predict the one after the last stop\n",
    "            continue\n",
    "\n",
    "        model = build_model(j+1)\n",
    "\n",
    "        data_train, data_test, targets_train, targets_test = train_test_split(info.x[j], info.y[j], test_size=0.2, random_state=42)\n",
    "\n",
    "        history = model.fit(data_train, targets_train,\n",
    "                            epochs = 100,\n",
    "                            shuffle = True,\n",
    "                            verbose = 0)\n",
    "        \n",
    "        tmp_models.append(model.get_weights())\n",
    "        \n",
    "    info.models = tmp_models.copy()\n",
    "    with open('../data/lines_info.pickle', 'wb') as file:\n",
    "        pickle.dump(lines_info, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
